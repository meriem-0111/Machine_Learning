#!/usr/bin/env python
# coding: utf-8

# In[1]:


import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

from sklearn.metrics import mean_squared_error,r2_score
import pandas as pnd


# <h1 style="color: red;">Section 1: Data</h1>

# <h2>1) Préparation de données</h2>

# In[2]:


np.random.seed(44) #à chaque exécution,générer le même dataset de manière aléatoire
# Coefficients
a1, a2, b = 2, 3, 5  # y = 2*X1 + 3*X2 + 5 + bruit
nombre_points = 100 # Nombre de points
# Génération des deux features (X1 et X2)
X1 = np.random.rand(nombre_points) * 10
X2 = np.random.rand(nombre_points) * 10
# Empilement des features dans une seule matrice (shape: (100, 2))
X = np.column_stack((X1, X2))
# Génération du bruit
bruit = np.random.randn(nombre_points) * 2  # Bruit
# Calcul de la target
y = a1 * X1 + a2 * X2 + b + bruit

#spilt data
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=23)


# <h1 style="color: red;">Section 2: Neural network avec tensorflow</h1>

# In[5]:


import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense


# <h2>2) Modèle de réseau de neurones</h2>

# In[5]:


model_nn = Sequential() #compéter
output_layer = Dense(1, activation='linear') #compéter
model_nn.add(output_layer)
model_nn.compile(optimizer='adam', loss='mse', metrics=['mse']) #compéter
model_nn.fit(X_train, y_train, epochs=100, batch_size=4, verbose=1) #compéter


# <h2>3) Prédiction en utilisant le modèle</h2>

# In[11]:


yhat_nn=model_nn.predict(X_test)


# In[34]:


yhat_nn=yhat_nn.flatten()


# In[35]:


W_nn, bias_nn = model_nn.layers[0].get_weights()

# Afficher les poids et biais
print("Poids :", W_nn.flatten())
print("Biais :", bias_nn)


# In[36]:


W,bias


# <h2>4) Evaluation du modèle</h2>

# In[37]:


mse_nn,r2_nn=mean_squared_error(yhat_nn,y_test),r2_score(yhat_nn,y_test)
print(mse_nn,r2_nn)

import matplotlib.pyplot as plt

# Tracé des résidus
plt.scatter(yhat_nn, y_test - yhat_nn, color='blue', alpha=0.6)
plt.axhline(0, color='red', linestyle='--')
plt.xlabel("Valeurs prédites (ŷ)")
plt.ylabel("Résidus (y - ŷ)")
plt.title("Graphique des résidus")
plt.grid(True)
plt.show()


# <h1>From scratch</h1>
# 

# <h1 style="color: red;"> Section 3 :Régression linéaire from scratch </h1>
# 

# <h2>Modèle (version1) de régression linéaire from scratch avec utilisation des matrices</h2>
# 

# In[48]:


learning_rate = 0.0001
epochs = 1000


# Initialisation des paramètres
W = np.array([[0.0], [0.0]])  # Shape: (2, 1)
b = 0.0

# Reshape y_train pour garantir les dimensions adéquates 
y_train = y_train.reshape(-1, 1)  # Shape: (n_samples, 1)
n = len(X_train)

# Entraînement (descente de gradient vectorisée)
for epoch in range(epochs):
    y_pred = X_train @ W + b #compéter, @ pour un produit entre matrices  # Shape: (n, 1)
    error = y_pred - y_train  # Shape: (n, 1)

    # Calcul des gradients
    dW = (1/n) * (X_train.T @ error) #compéter  # Shape: (2, 1)
    db = (1/n) * np.sum(error)        # Scalaire

    # Mise à jour des paramètres
    W -= learning_rate * dW #compéter
    b -= learning_rate * db

# Résultats
print("Paramètres ajustés:")
print(f"W = \n{W}")
print(f"b = {b:.4f}")


# In[ ]:





# In[ ]:




